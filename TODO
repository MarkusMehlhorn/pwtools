base.py
-------
* Re-think the whole implementation of FlexibleGetters. I think all of this can
  be done using lazy evaluation, i.e. decorate getters in derived classed with
  something like "@lazyproperty". Then the actual calculation is done only
  once, if a attr is accessed (i.e. struct.cell). Currently we do that with
  calls to check_set_attr() in each getter. Some ideas: 
  http://stackoverflow.com/questions/3012421/python-lazy-property-decorator
  Then, FlexibleGetters can be removed completely!
  Instead of lazyproperty, ordinary properties might suffice.

parse.py
--------
* Many classes still call the base classes' __init__ explicitly. We get unbound
  method errors in interactive Ipython sometimes if we reload modules. We need
  to use use super() instead. But that only calls *one* __init__, namely the
  one from the major base class::

    class C(B,A):
        pass
    # calls only B.__init__, not A.__init__    
    c=C()
  
  In some classes, we call __init__'s of two base classes and cannot use
  super() then? Is this bad design? Probably, since the MRO (method resolution
  order) has clear rules also for multi inheritance. But who understands them?
* Speed: We did some profiling using kernprof.py on Cp2kMDOutputFile, which
  uses np.fromstring(backtick(cmd)).reshape(...). We found that the Python
  overhead for this command compared to executing all the grep/sed/awk stuff in
  `cmd` directly in the shell is small (about 10% top). So, the rest of
  slowdown comes from stuff which we do in pure Python with the parsed arrays
  (esp. in Trajectory, I guess, which calls crys.*3d() a lot).

crys.py
-------
* All *3d() funcs use simple loops currently. This is slow. Find vectorized
  versions or re-implement in Fortran (flib.f90) / Cython. But use profiling
  first! Ipython's %prun or kernprof.py / line profiler.
* Structure / Trajectory: Instead of cryst_const, store abc+angles, apply
  length unit to abc and derive cell + cryst_const from abc+angles. 
* rpdf(): Use scipy.spatial.distance.pdist() or cdist() or a variant of
   _flib.distsq_frac() for 3d array which is *much* faster.
  pdist() also returns a "condensed" 1d array w/ all distances. We can feed
  that directly to histogram()! This would solve major problems with that
  fuction.
* rpdf(): If we re-code the distance part in Fortran, add support for variable
  cell trajectories, if the rpdf is defined for that. Makes surely no sense if
  the cell changes drastically during an MD and we do the average of the rpdf's 
  over time? Also, rmax will change in each timestep! We'll then output only up
  to the smallest of all rmax values.
* Maybe make crys.Structure / Trajectory do lazy evaluation by default 
  by setting set_all_auto to False (maybe rename it to "lazy"). No we have:
    st = Structure(...)
    st.cell         # access pre-calculated attr
    st.get_cell()   # calculate and return or return pre-calculated attr
  We want:  
    st.cell # call self.get_cell() if not calculated etc.
  This can be done only if we turn all attrs into lazy evaluated properties.

pydos.py
--------
* let *_pdos() return one 2d array with several columns, like crys.rpdf()
* pydos is a very stupid name :)
 
all
---
* Drop verbose.py. Use the logging or warnings module.
* rename all functions foo3d() -> foo_traj(). But not funcs which take 3d arrays
  directly. Only those which take a Trajectory as argument.

examples
--------
Clean up examples/phonon_dos/pdos_methods.py .

constants.py / units in general
-------------------------------
* Maybe change pressure (GPa currently) and time step (fs = 1e-15 sec) units to
  ASE values?? Would probably break a lot of eval scripts. Maybe to ease the
  transition, first introduce apu (atomic pressure unit = GPa, later ASE value
  = eV / Ang**3), atu (atomic time unit = fs, later ASE value = ???) in
  constants.py and use that everywhere instead of assuming GPa and fs.
  Set apu_to_GPa =1.0 for now etc...
* The units implementation is a mess. Now, we have UnitsHandler ->
  StructureFileParser -> parse.*OutputFile and default_units in each 
  *OutputFile. This must be simplified. 

flib.f90
--------
* vacf() uses (natoms, 3, nstep) instead of (nstep,natoms,3). We take care of
  that in the wrapper pydos.fvacf() but it would be nice to have a consistent
  implementation.

num.py
------
* Similar to the Fit1D base class, we could also define a FitND base class
  which defines get_min() for the ND-case. Right now we have the same get_min()
  implementation in PolyFit and Interpol2D. We could derive them from FitND. 
* Interpol2D is actually InterpolND if we remove the 'bispl' method. So add
  InterpolND and derive Interpol2D from that and add bispl only there. Uuuuhh :)

eos.py, thermo.Gibbs
--------------------
* Make the EOS fit classes have the same API as num.Fit1D: get_min(),
  get_root(), __call__() accepts `der` kwd.
* Rename get_min() [which returns a dict {v0, e0, p0, b0}] to smth like
  get_eos_min(). Make get_min() behave like in num.Fit1D and maybe even derive
  from that. In get_min(), return v0 from the fit instead of doing a
  minimization. The only difference should be that there is an additional
  get_eos_min() and that the fit function contains some physics. This would
  make it *much* easier to use these EOS fit classes in thermo.Gibbs. Right now
  people use smth like

    >>> def fit_1d_G(x,y):
    ...     efit = eos.ElkEOSFit(energy=y, volume=x, verbose=False)
    ...     efit.fit()
    ...     return efit.spl_ev
    >>> gibbs.set_fitfunc('1d-G', fit_1d_G)
  
  which uses a spline thru the fitted data points and then we use the Spline's
  get_min(). But why not use the EOS's v0, b0, ... values, which are the result
  of the EOS fit anyway? ATM all our use cases used PolyFit1D b/c we have
  enough data points. But other people with big systems can only afford some
  E(V) points and then an EOS is better. Data scaling is mandatory before
  fitting here, too! We also did not use eos.x for 1D QHA b/c the min position showed
  small scatter. According to Thomas' tests, his Vinet class uses data scaling
  (as we suggested, do it like we do in PolyFit) and so the scatter is gone.
  The best would be if someone who really needs this would implement a proper Vinet
  class + a test! ATM people use the default polys or Thomas' patch.
* Add own implementation at least for the Vinet EOS. This ought to be much
  faster than calling an external tool. AFAIR, one needed to do some hacks with
  the start guess and data scaling was problematic somehow.
* Remove all call-external-fortnan-eos-fit-tool classes, which was not such a
  good idea. Implementing this is really trivial and much easier in the end.
  Only once generate test data with an external tool.
  Have a look here:
  http://pymatgen.org/_modules/pymatgen/io/abinitio/eos.html

mpl.py
------
* Farm this out to the mplextra package. We already placed the set_plot_*
  functions there.

# vim:comments=fb\:*
